{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"kernal running...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accent removal\n",
    "import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,math,re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(fname):\n",
    "    myf = open(fname,\"rb\")\n",
    "    text = myf.read().decode(errors='replace')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tokens(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sw_remove(word_tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(new_tokens):\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = []\n",
    "    for i in new_tokens:\n",
    "        stemmed.append(ps.stem(i))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----stats---------\n",
      "text:  29387\n",
      "tokens:  3853\n",
      "tokens after preprocessing: 2701\n",
      "------------------\n",
      "              0\n",
      "0       nebular\n",
      "1        region\n",
      "2         south\n",
      "3          zeta\n",
      "4        orioni\n",
      "5            37\n",
      "6         photo\n",
      "7             :\n",
      "8         mount\n",
      "9        wilson\n",
      "10  observatori\n",
      "11            .\n",
      "12         star\n",
      "13      cluster\n",
      "14       hercul\n"
     ]
    }
   ],
   "source": [
    "# Data-Preprocessing\n",
    "\n",
    "filename = \"mydata/Text1.txt\"\n",
    "text = get_corpus(filename)\n",
    "\n",
    "print(\"\\n\\n----stats---------\")\n",
    "print(\"text: \", len(text))\n",
    "\n",
    "tokens = get_word_tokens(text)\n",
    "print(\"tokens: \", len(tokens))\n",
    "\n",
    "tokens = sw_remove(tokens)\n",
    "tokens = stem_tokens(tokens)\n",
    "\n",
    "print(\"tokens after preprocessing:\", len(tokens))\n",
    "print(\"------------------\")\n",
    "\n",
    "print(pd.DataFrame(tokens).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index to get Indexes of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T1.txt', 'T2.txt', 'T3.txt', 'T4.txt', 'T5.txt', 'T6.txt', 'T7.txt', 'T8.txt', 'T9.txt', 'T10.txt']\n"
     ]
    }
   ],
   "source": [
    "#List of files\n",
    "files = [r'mydata/Inverted Index/T1.txt',\n",
    "         r'mydata/Inverted Index/T2.txt',\n",
    "         r'mydata/Inverted Index/T3.txt',\n",
    "         r'mydata/Inverted Index/T4.txt',\n",
    "         r'mydata/Inverted Index/T5.txt',\n",
    "         r'mydata/Inverted Index/T6.txt',\n",
    "         r'mydata/Inverted Index/T7.txt',\n",
    "         r'mydata/Inverted Index/T8.txt',\n",
    "         r'mydata/Inverted Index/T9.txt',\n",
    "         r'mydata/Inverted Index/T10.txt',]\n",
    "# xfiles = [(i[len(i)-i[::-1].index('/'):]) for i in files ]\n",
    "xfiles = ['T1.txt', 'T2.txt', 'T3.txt', 'T4.txt', 'T5.txt', 'T6.txt', 'T7.txt', 'T8.txt', 'T9.txt', 'T10.txt']\n",
    "print(xfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for inv_ind()\n",
    "class CreateInvDict:\n",
    "    def __init__(self):\n",
    "        self.myd = {}\n",
    "    def checkf(self, x, i):\n",
    "        if i not in self.myd.keys():\n",
    "            self.myd[i] = [x]\n",
    "        else:\n",
    "            self.myd[i].append(x)\n",
    " \n",
    "def freq_list(str,word):\n",
    "    count = str.count(word)\n",
    "    mid = -1\n",
    "    freq = []\n",
    "    for i in range(count):\n",
    "        prev = str[mid+1:].index(word)\n",
    "        mid += (prev+1)\n",
    "        freq.append(mid)\n",
    "    return freq\n",
    " \n",
    "def freq_count(text, word):\n",
    "    return text.count(word)\n",
    " \n",
    "#Inverted index, return dictionary\n",
    "def inv_ind(stemmed_docs,doc_sizes,n):\n",
    "    unq_tok = set(stemmed_docs)\n",
    "    inv_table = CreateInvDict()\n",
    "    for i in unq_tok:\n",
    "        start = 0\n",
    "        for j in range(n):\n",
    "            end = doc_sizes[j]\n",
    "            temp = stemmed_docs[start:(start+end)]\n",
    "            if i in temp:\n",
    "                x = (xfiles[j],freq_count(temp,i))\n",
    "                inv_table.checkf(x,i)\n",
    "            start += end\n",
    "    return inv_table.myd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(files)\n",
    "doc_sizes = []\n",
    "stemmed = []\n",
    "for i in files:\n",
    "    text = get_corpus(i)\n",
    "    tokens = get_word_tokens(text)\n",
    "    tokens = sw_remove(tokens)\n",
    "    tokens = stem_tokens(tokens)\n",
    "    doc_sizes.append(len(tokens))\n",
    "    stemmed.extend(tokens)\n",
    "\n",
    "table = inv_ind(stemmed,doc_sizes,n)\n",
    "df = pd.DataFrame(table.items(), columns=['Tokens','Occurences'])\n",
    "try:\n",
    "    os.remove(r'Inverted.csv')\n",
    "except:\n",
    "    pass\n",
    "df.to_csv(r'Inverted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_file = r'Inverted.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance(n,nw):\n",
    "  return (n-nw+0.5)/(nw+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_matrix(n, df, toks):\n",
    "  prob_matrix = {}\n",
    "\n",
    "  for i in toks:\n",
    "    nw = df.loc[i, 'Occurences'].count(')')     #number of documents where current token is present\n",
    "    prob_matrix[i] = [nw, get_relevance(n,nw)]\n",
    "  return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_tokens(query):\n",
    "  tokens = get_word_tokens(query.lower())\n",
    "  tokens = sw_remove(tokens)\n",
    "  tokens = stem_tokens(tokens)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_probability(qtok, inv_file):\n",
    "  prob_matrix = {}\n",
    "  df = pd.read_csv(inv_file)\n",
    "  toks = list(df['Tokens'])\n",
    "\n",
    "  df.set_index('Tokens',inplace=True)\n",
    "  word_matrix = get_prob_matrix(len(xfiles), df, toks)\n",
    "#   print(word_matrix)\n",
    "#   print(len(xfiles), xfiles)\n",
    "#   for i in xfiles:\n",
    "#     print(i)\n",
    "  for i in xfiles:\n",
    "    flag = False\n",
    "    val = 1\n",
    "    prob_matrix[i] = 0\n",
    "#     print(i)\n",
    "    for j in qtok:    # for each query tokend\n",
    "      if j in toks:   # if current query token is in document tokens\n",
    "        if i in df.loc[j,'Occurences']:  # if current doucment is present in list of inverted index querytoken  \n",
    "          flag = True\n",
    "          val *= word_matrix[j][1]   # multiply because of |and| when multiple tokens present in this file\n",
    "    prob_matrix[i] = val if flag else 0 #if val=1 that means the query token is not present in document tokens so 0 probability\n",
    "    \n",
    "#     print(prob_matrix)\n",
    "  return prob_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the query, spaces are considered as |and| because of tokensizing and searching the relavant words in document, because of and the probability of each query token is multiplied\n",
      "moon sun\n",
      "      File                                   Relevance\n",
      "0   T2.txt  0.2941176470588235392078502172807930037379\n",
      "1   T6.txt  0.2941176470588235392078502172807930037379\n",
      "2   T1.txt  0.2036199095022624416806422686931909993291\n",
      "3   T3.txt  0.2036199095022624416806422686931909993291\n",
      "4   T4.txt  0.2036199095022624416806422686931909993291\n",
      "5   T5.txt  0.2036199095022624416806422686931909993291\n",
      "6   T9.txt  0.2036199095022624416806422686931909993291\n",
      "7  T10.txt  0.2036199095022624416806422686931909993291\n",
      "8   T7.txt  0.0000000000000000000000000000000000000000\n",
      "9   T8.txt  0.0000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter the query, spaces are considered as |and| because of tokensizing and searching the relavant words in document, because of and the probability of each query token is multiplied\")\n",
    "vect = get_cond_probability(get_query_tokens(input()),inv_file)\n",
    "vect = {k: \"{0:.40f}\".format(v) for k, v in sorted(vect.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(pd.DataFrame(vect.items(),columns=['File','Relevance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
